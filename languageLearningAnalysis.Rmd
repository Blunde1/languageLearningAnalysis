---
title: "Analyse av language-learning data"
output:
  html_document:
    code_folding: hide
abstract: "
  Dokumentet tar utgangspunkt i målingene fra language-learning prosjektet.
  Analysene er gjort under antagelse av at forsøkspersonene er samplet fra samme initiell fordeling.
  Denne antagelsen kan diskuteres. 
  Resultatet av målingene blir først presentert visuelt, for så å bli analysert med statistiske tester på marginalt nivå.
  Null-hypotesen, at forventningen er lik for LIN og Helsfyr for en gitt måling, blir ikke forkastet når hver dimensjonene <finn bedre ord> er testet hver for seg.
  Av de åtte målingene så er resultatene fra \\<insert-measurement-type\\> de resultatene som er nærmest å forkaste en individuell null-hypotese.
  Dette blir klart etter inspeksjon av differanser etter en standardisering av de ulike dimensjonene, see Figur \\<sett inn fig-number\\>.
  Merk likevel at strengere krav til signifikans må stilles når flere hypoteser blir testet samtidig, dette grunnet multippel-testing-problem \\<sett-inn-en-bra-referanse\\>.
  Denne standardiseringen viser at alle gjennomsnitt for progresjon i de ulike dimensjonene i Helsfyr er mindre enn korresponderende gjennomsnitt ved LIN.
  En statistisk test er utviklet for å teste en null-hypotese (at summen av standardiserte målinger er like) mot en alternativ (at de er ulike / Helsfyr < LIN).
  Denne testen forkaster/forkaster ikke null hypotesen på <konfidens>-nivå.
  Dette er et første resultat på at språklig progresjon er lik/ulik for de to ulike skolene.
  Følgelig motiverer det til økt interesse, spørsmålsstilling og forskning rundt status-quo i norsk språkundervisningen.
  "
---

### Målinger og data
I tabellen under er data fra Tabell 1 til Tabell 8 insatt.
Denne er utgangspunktet for analysene som vil bli gjort.
Merk at "Helsfyr" er forkortet til "HLF". Dette er en konvensjon for resten av dokumentet.

Dokumentet snakker om forventninger, som er matematisk definert for en tilfeldig variabel, $X$, ved $E[X]=\int xdP_X(x)$ der $E[X]$ og $P_X(x)$ er forventningen og den kumulative fordelingen til $X$.
For den ikke-matematiske, så kan man tenke på forventningen som det _underliggende og strukturelle_ til en gruppe observasjoner -- nemlig det som vil vere gjennomsnittet gitt at man har mange nok observasjoner.

En antagelse som er gjort for alle testene som nå blir gjort er at forsøkspersonene er samplet fra den samme fordelingen. Dette er en antagelse som diverre trolig ikke holder, og som er diskutert i seksjon \<insert-relevant-section-in-main-document\>.
Målingene viser differanser (see seksjon \<section\> for detaljer).
En naturlig antagelse for forventningen til en språklig kunnskapskurve, er at denne er strengt økene i konstant og uniform læring over tid.
Dette medfører at forventningen til en differanse i kunnskap (altså læring) av typen i tabellen under ikke skal vere negativ. Dette er heller ikke tilfelle.
En naturlig antagelse til for forventningen til en språklig læringskurve (differanse i kunnskap), er at differansen i læring er avtagende gitt høyere kunnskap. 
Dette er av praktisk betydning, fordi forsøkspersonene ved ved LIN var analfabeter, mens de ved Helsfyr i stor grad ikke var dette. 
Siden utgangspunktet ikke er likt, er det ikke gitt at det er en naturlig null hypotese å anta at forventningen i læring (bestemt av en måling i tabellen under) er lik for Helsfyr og LIN. 
En mer naturlig null-hypotese er at forsøkspersonene ved LIN har større forventet læring enn forsøkspersonene ved Helsfyr.
Begge disse hypotesene vil bli testet.

Et interessant spørsmål er hva det totale bildet av alle målingene sier.
Statistisk sett er dette et komplisert spørsmål, av den enkle grunn at _de ulike dimensjonene ikke kan antas å vere uavhengige_. 
I den siste seksjonen i dokumentet støtter vi oss på at gruppe-målingene er gjennomsnitt, og vil ha en fordeling som tenderer mot en normalfordeling.
Når målingene er standardiserte og i så måte har lik skala, kan vi stille spørsmål rundt summen av gjennomsnittene for LIN og HLF. 
Normalfordelingen sin kovariansmatrise hjelper oss å ta hensyn til avhengigheten mellom dimensjonene.

Mye statistisk teori baserer seg på at funksjoner tenderer mot (ofte kvadratiske) standard funksjonsuttrykk jo mer data vi har. 
For eksempel, så vil et gjennomsnitt vere eksakt normalfordelt (Gaussisk fordeling) når vi har uendelig mye data.
I language-learning prosjektet har vi 5 observasjoner fra HLF og 7 fra LIN.
Diverre er det slik at $5<7<\infty$.
Å benytte tendensen mot normalfordeling for fordelingen til gjennomsnitt blir likevel ofte sett på som grei når vi har mer enn 30 observasjoner. 
Dette har vi dessverre ikke for hver enkelt måling, men hvor mange _effektive observasjoner_ vi har for det det totale antall målinger er uklart. 
Men det vil ligge et sted mellom `12` (målingene er fullstendig avhengige) og `r `12*96` (målingene er uavhengige).
Dette vil vi estimere.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(dplyr)
library(ggplot2)
library(lazyeval) # interp function
library(purrr)
library(kableExtra)
```

```{r create-data}
llData <- data.frame(
  "School" = c(rep("LIN", 7), rep("HLF", 5)),
  "Table 1" = c(0.225, NA, 0.1, 0.375, 0.4, NA, 0.067,
                0.175, 0.435, 0.1, 0.067, 0.175),
  "Table 2" = c(2.2224, 4, 0.2847, 2.0412, 2.7784, 2.1429, 1.1763, 
                0.6131, 4.0275, 0.4979, 1.5455, 2.2944),
  "Table 3" = c(0.3093, 1.3091, 0.2917, 0.1513, 0.0848, 1.3636, -0.2113, 
                0.0086, 0.3527, -0.0459, -2.2622, 0.1770),
  "Table 4" = c(-0.1143, 0.6246, -0.2732, 0.2853, -0.2179, 0.7273, 0.1292,
                0.0933, 0.0573, 0.3067, 0.1345, 0.1364),
  "Table 5" = c(0.0, 0.9231, 0.0833, 0.1207, -0.0133, 1.0, 0.1865,
                0.2188, 0.0903, 0.3497, 0.1364, 0.0357),
  "Table 6" = c(-0.0101, 0.1111, -0.0088, 0.0722, 0.0293, 0.1333, -0.0339,
                0.0026, 0.0110, 0.0597, 0.0060, 0.0158),
  "Table 7" = c(-0.1367, 0.6, 0.0802, 0.2619, 0.1372, 0.75, 0.2619,
                0.2111, 0.0058, 0.4502, 0.0756, 0.1152),
  "Table 8" = c(-0.0909, 0.4545, 0.2323, 0.31, 0.147, 0.5833, 0.2148,
                0.2515, 0.1, 0.375, 0.0365, 0.1909)
)
names(llData) = c("school", "T1", "T2", "T3", "T4", "T5", "T6", "T7", "T8")
llData <- llData %>% mutate(school=factor(school, levels=unique(school)))
test_cols <- names(llData)[-1]
```


```{r look-at-wide-data}
llData %>%
    kbl(caption = "Measurement-data for the students at LIN and HLF. See <insert-reference-here> for a detailed explanation of the measurement methodology.",
        title = "Measurement-data") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


### Marginale hypotesetester

\<insert parallel coordinate plot\>

Anta at gruppe $A$ er HLF og gruppe $B$ er LIN.
La $\mu=E[X]$ vere forventningen til en variabel.
Den klassiske statistiske testen for å teste

- $H0: \mu_A = \mu_B$,

mot 

- $H1: \mu_A \neq \mu_B$,

er en t-test \<insert-reference\>.
t-testen blir kalt en Welch's t-test når vi har ulikt antall observasjonar i hver gruppe, $n_A\neq n_B$, og når underliggande varians for kvar gruppe er ulik og må bli estimert.
Dette er tilfelle for våre data -- vi ønsker ikke å gjøre unødvendige antagelser om for eksempel lik varians.

En t-test har antagelser om normalitet som ikke kan antas å holde når vi har så lite data i hver gruppe, $n_A=5$ og $n_B=7$.
For å lette på antagelsene om normalitet tester vi i tillegg til klassisk Welch's t-test en parametrisk bootstrap Welch t-test \<insert-reference\>. 

\<insert $3\times 4$ figure with individual radial plots\>

\<insert table with marginal hypothesis testing, both two-sided and one-sided\>

\<insert figure with scaled t-test confidence bands + group-means in radial plot\>


### Hypotesetester for summen av målinger

> Explain the naive independence assumption

> Explain not just HLF < LIN 0.5^8 (loose information about distance)

> Do the math for the distribution of statistic under null-hypothesis (sum of averages are equal)

> Clearly define the assumptions and estimators etc.

\<insert table for hypothesis test for sum of measurements\>


### Conclusion

Write amazing conclusion
